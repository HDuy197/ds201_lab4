{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cài đặt thư viện"
      ],
      "metadata": {
        "id": "PaO16lTHw7FV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wxvoQQqBcPXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a2aa4ef-c6e6-4cbd-d6d3-0463e016a5a5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate sacrebleu rouge_score\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import evaluate\n",
        "from collections import Counter\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cấu hình"
      ],
      "metadata": {
        "id": "PsTK1_Fkw5eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "TRAIN_PATH = 'small-train.json'\n",
        "DEV_PATH   = 'small-dev.json'\n",
        "TEST_PATH  = 'small-test.json'\n",
        "\n",
        "HIDDEN_SIZE = 256\n",
        "N_LAYERS    = 3\n",
        "DROPOUT     = 0.5\n",
        "BATCH_SIZE  = 32\n",
        "LR          = 1e-3\n",
        "N_EPOCHS    = 30"
      ],
      "metadata": {
        "id": "f2eF7eZHck9Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xử lý data"
      ],
      "metadata": {
        "id": "k03FcBy8xEwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, frequency_threshold=1):\n",
        "        self.itos = {0: \"<pad>\", 1: \"<bos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
        "        self.stoi = {\"<pad>\": 0, \"<bos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.freq_threshold = frequency_threshold\n",
        "        self.pad_idx = 0\n",
        "        self.bos_idx = 1\n",
        "        self.eos_idx = 2\n",
        "        self.unk_idx = 3\n",
        "\n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "        for word, count in frequencies.items():\n",
        "            if count >= self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "        return len(self.itos)\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.stoi.get(token, self.unk_idx) for token in self.tokenize(text)]\n",
        "\n",
        "class VocabConfig:\n",
        "    def __init__(self, src_vocab, tgt_vocab):\n",
        "        self.total_src_tokens = len(src_vocab)\n",
        "        self.total_tgt_tokens = len(tgt_vocab)\n",
        "        self.pad_idx = src_vocab.pad_idx\n",
        "        self.bos_idx = src_vocab.bos_idx\n",
        "        self.eos_idx = src_vocab.eos_idx\n",
        "        self.unk_idx = src_vocab.unk_idx\n",
        "\n",
        "class PhoMTDataset(Dataset):\n",
        "    def __init__(self, json_file, limit=None):\n",
        "        self.data = []\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        if limit: self.data = self.data[:limit]\n",
        "        self.vocab_src = None\n",
        "        self.vocab_tgt = None\n",
        "\n",
        "    def build_vocabs(self):\n",
        "        self.vocab_src = Vocab(1)\n",
        "        self.vocab_tgt = Vocab(1)\n",
        "        src_texts = [item['english'] for item in self.data]\n",
        "        tgt_texts = [item['vietnamese'] for item in self.data]\n",
        "        self.vocab_src.build_vocab(src_texts)\n",
        "        self.vocab_tgt.build_vocab(tgt_texts)\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        src_encoded = self.vocab_src.encode(item['english'])[::-1]\n",
        "        tgt_encoded = self.vocab_tgt.encode(item['vietnamese'])\n",
        "        src_indices = [self.vocab_src.bos_idx] + src_encoded + [self.vocab_src.eos_idx]\n",
        "        tgt_indices = [self.vocab_tgt.bos_idx] + tgt_encoded + [self.vocab_tgt.eos_idx]\n",
        "\n",
        "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx): self.pad_idx = pad_idx\n",
        "    def __call__(self, batch):\n",
        "        src = [item[0] for item in batch]\n",
        "        trg = [item[1] for item in batch]\n",
        "        src = pad_sequence(src, batch_first=True, padding_value=self.pad_idx)\n",
        "        trg = pad_sequence(trg, batch_first=True, padding_value=self.pad_idx)\n",
        "        return src, trg"
      ],
      "metadata": {
        "id": "3i77jQzIcmdC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tải data"
      ],
      "metadata": {
        "id": "qRWi62BUxGf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PhoMTDataset(TRAIN_PATH, limit=20000)\n",
        "dev_dataset = PhoMTDataset(DEV_PATH, limit=2000)\n",
        "test_dataset = PhoMTDataset(TEST_PATH, limit=2000)\n",
        "\n",
        "train_dataset.build_vocabs()\n",
        "\n",
        "dev_dataset.vocab_src = train_dataset.vocab_src\n",
        "dev_dataset.vocab_tgt = train_dataset.vocab_tgt\n",
        "test_dataset.vocab_src = train_dataset.vocab_src\n",
        "test_dataset.vocab_tgt = train_dataset.vocab_tgt\n",
        "\n",
        "train_dataset.data.sort(key=lambda x: len(x['english'].split()))\n",
        "dev_dataset.data.sort(key=lambda x: len(x['english'].split()))\n",
        "test_dataset.data.sort(key=lambda x: len(x['english'].split()))\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Dev: {len(dev_dataset)} | Test: {len(test_dataset)}\")\n",
        "\n",
        "pad_idx = train_dataset.vocab_src.pad_idx\n",
        "collate_fn = MyCollate(pad_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "vocab_config = VocabConfig(train_dataset.vocab_src, train_dataset.vocab_tgt)"
      ],
      "metadata": {
        "id": "WaW45BF3cn_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ec508c-421c-4c7c-8530-1778c81a41b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 20000 | Dev: 2000 | Test: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mô hình"
      ],
      "metadata": {
        "id": "UH9t0XY7xOkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=True, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_hidden = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.fc_cell = nn.Linear(hid_dim * 2, hid_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        n_layers_2, batch, hid = hidden.shape\n",
        "        n_layers = n_layers_2 // 2\n",
        "\n",
        "        hidden = hidden.view(n_layers, 2, batch, hid)\n",
        "        cell = cell.view(n_layers, 2, batch, hid)\n",
        "\n",
        "        hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)\n",
        "        cell = torch.cat((cell[:, 0, :, :], cell[:, 1, :, :]), dim=2)\n",
        "\n",
        "        hidden = torch.tanh(self.fc_hidden(hidden))\n",
        "        cell = torch.tanh(self.fc_cell(cell))\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((hid_dim * 2) + hid_dim, hid_dim)\n",
        "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM((hid_dim * 2) + emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        a = self.attention(hidden[-1], encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=2))\n",
        "        prediction = prediction.squeeze(1)\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            top1 = output.argmax(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def predict(self, src, max_len=50, bos_idx=1, eos_idx=2):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            batch_size = src.shape[0]\n",
        "            encoder_outputs, hidden, cell = self.encoder(src)\n",
        "            input = torch.tensor([bos_idx] * batch_size).to(self.device)\n",
        "            outputs = []\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "                pred_token = output.argmax(1)\n",
        "                outputs.append(pred_token.unsqueeze(1))\n",
        "                input = pred_token\n",
        "\n",
        "            return torch.cat(outputs, dim=1)"
      ],
      "metadata": {
        "id": "yLtpTCV7cp5y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tạo Model"
      ],
      "metadata": {
        "id": "LufbbrFCxQab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn = Attention(HIDDEN_SIZE)\n",
        "\n",
        "enc = Encoder(vocab_config.total_src_tokens, HIDDEN_SIZE, HIDDEN_SIZE, N_LAYERS, DROPOUT)\n",
        "\n",
        "dec = Decoder(vocab_config.total_tgt_tokens, HIDDEN_SIZE, HIDDEN_SIZE, N_LAYERS, DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name and param.dim() > 1:\n",
        "            nn.init.xavier_uniform_(param.data)\n",
        "        elif 'bias' in name:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab_config.pad_idx)"
      ],
      "metadata": {
        "id": "XHJtzFX_cuTY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traning"
      ],
      "metadata": {
        "id": "zLOJIuq6xVwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "print(f\"Huấn luyện {N_EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch {epoch+1:02} | Time: {end_time - start_time:.0f}s | Train Loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zfcns4wcv0t",
        "outputId": "27d9296a-e5a3-4353-e902-bbca8b017bff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Huấn luyện 30 epochs\n",
            "Epoch 01 | Time: 93s | Train Loss: 5.8139\n",
            "Epoch 02 | Time: 92s | Train Loss: 5.2471\n",
            "Epoch 03 | Time: 92s | Train Loss: 4.8531\n",
            "Epoch 04 | Time: 93s | Train Loss: 4.5345\n",
            "Epoch 05 | Time: 91s | Train Loss: 4.3053\n",
            "Epoch 06 | Time: 90s | Train Loss: 4.1069\n",
            "Epoch 07 | Time: 90s | Train Loss: 3.9250\n",
            "Epoch 08 | Time: 90s | Train Loss: 3.7923\n",
            "Epoch 09 | Time: 88s | Train Loss: 3.6353\n",
            "Epoch 10 | Time: 89s | Train Loss: 3.5035\n",
            "Epoch 11 | Time: 89s | Train Loss: 3.4203\n",
            "Epoch 12 | Time: 89s | Train Loss: 3.3174\n",
            "Epoch 13 | Time: 89s | Train Loss: 3.2373\n",
            "Epoch 14 | Time: 89s | Train Loss: 3.1606\n",
            "Epoch 15 | Time: 90s | Train Loss: 3.0917\n",
            "Epoch 16 | Time: 89s | Train Loss: 3.0190\n",
            "Epoch 17 | Time: 90s | Train Loss: 2.9661\n",
            "Epoch 18 | Time: 90s | Train Loss: 2.9153\n",
            "Epoch 19 | Time: 90s | Train Loss: 2.8628\n",
            "Epoch 20 | Time: 89s | Train Loss: 2.8104\n",
            "Epoch 21 | Time: 89s | Train Loss: 2.7807\n",
            "Epoch 22 | Time: 89s | Train Loss: 2.7437\n",
            "Epoch 23 | Time: 91s | Train Loss: 2.6924\n",
            "Epoch 24 | Time: 89s | Train Loss: 2.6397\n",
            "Epoch 25 | Time: 88s | Train Loss: 2.5943\n",
            "Epoch 26 | Time: 88s | Train Loss: 2.5599\n",
            "Epoch 27 | Time: 89s | Train Loss: 2.5161\n",
            "Epoch 28 | Time: 88s | Train Loss: 2.4945\n",
            "Epoch 29 | Time: 88s | Train Loss: 2.4723\n",
            "Epoch 30 | Time: 88s | Train Loss: 2.4428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đánh giá"
      ],
      "metadata": {
        "id": "QU0WBoA4xXnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def calculate_rouge(model, loader, dataset):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    refs = []\n",
        "\n",
        "    print(\"Tính toán ROUGE-L trên tập Test\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(loader):\n",
        "            src = src.to(DEVICE)\n",
        "\n",
        "            batch_preds = model.predict(src)\n",
        "\n",
        "            bs = src.shape[0]\n",
        "\n",
        "            if batch_preds.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            for i in range(bs):\n",
        "                if i >= len(batch_preds): break\n",
        "\n",
        "                pred_tokens = []\n",
        "                for idx in batch_preds[i]:\n",
        "                    if idx == dataset.vocab_tgt.eos_idx: break\n",
        "                    pred_tokens.append(dataset.vocab_tgt.itos[idx.item()])\n",
        "\n",
        "                tgt_tokens = []\n",
        "                for idx in tgt[i]:\n",
        "                    if idx == dataset.vocab_tgt.eos_idx: break\n",
        "                    if idx not in [dataset.vocab_tgt.bos_idx, dataset.vocab_tgt.pad_idx]:\n",
        "                        tgt_tokens.append(dataset.vocab_tgt.itos[idx.item()])\n",
        "\n",
        "                preds.append(\" \".join(pred_tokens))\n",
        "                refs.append(\" \".join(tgt_tokens))\n",
        "\n",
        "    results = rouge.compute(predictions=preds, references=refs)\n",
        "    return results\n",
        "\n",
        "scores = calculate_rouge(model, test_loader, test_dataset)\n",
        "print(f\"KẾT QUẢ BÀI 2 (ROUGE-L): {scores['rougeL']:.4f}\")"
      ],
      "metadata": {
        "id": "5UJF4idccxPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3d2172-6012-46ab-aac4-4cd4e85a9428"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tính toán ROUGE-L trên tập Test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [00:06<00:00,  9.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KẾT QUẢ BÀI 2 (ROUGE-L): 0.3799\n"
          ]
        }
      ]
    }
  ]
}