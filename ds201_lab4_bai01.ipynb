{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cài đặt thư viện"
      ],
      "metadata": {
        "id": "2-wad2odc9tk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ewSV3j1rHgGF",
        "outputId": "7f11d202-0783-4949-e8bc-528702b2d883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate sacrebleu rouge_score\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import evaluate\n",
        "from collections import Counter\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVHADvMNRHhT"
      },
      "source": [
        "Cấu hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDRO8cp3NbP2"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "TRAIN_PATH = 'small-train.json'\n",
        "DEV_PATH   = 'small-dev.json'\n",
        "TEST_PATH  = 'small-test.json'\n",
        "\n",
        "HIDDEN_SIZE = 256\n",
        "N_LAYERS    = 3\n",
        "DROPOUT     = 0.5\n",
        "BATCH_SIZE  = 64\n",
        "LR          = 1e-3\n",
        "N_EPOCHS    = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvNife73RYse"
      },
      "source": [
        "Xử lý data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQimXfImORM2"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, frequency_threshold=1):\n",
        "        self.itos = {0: \"<pad>\", 1: \"<bos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
        "        self.stoi = {\"<pad>\": 0, \"<bos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.freq_threshold = frequency_threshold\n",
        "        self.pad_idx = 0\n",
        "        self.bos_idx = 1\n",
        "        self.eos_idx = 2\n",
        "        self.unk_idx = 3\n",
        "\n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "        for word, count in frequencies.items():\n",
        "            if count >= self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "        return len(self.itos)\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.stoi.get(token, self.unk_idx) for token in self.tokenize(text)]\n",
        "\n",
        "class PhoMTDataset(Dataset):\n",
        "    def __init__(self, json_file, limit=None):\n",
        "        self.data = []\n",
        "        if os.path.exists(json_file):\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                self.data = json.load(f)\n",
        "        if limit: self.data = self.data[:limit]\n",
        "\n",
        "        self.vocab_src = None\n",
        "        self.vocab_tgt = None\n",
        "\n",
        "    def build_vocabs(self):\n",
        "        self.vocab_src = Vocab(frequency_threshold=1)\n",
        "        self.vocab_tgt = Vocab(frequency_threshold=1)\n",
        "\n",
        "        src_texts = [item['english'] for item in self.data]\n",
        "        tgt_texts = [item['vietnamese'] for item in self.data]\n",
        "\n",
        "        self.vocab_src.build_vocab(src_texts)\n",
        "        self.vocab_tgt.build_vocab(tgt_texts)\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        src_encoded = self.vocab_src.encode(item['english'])[::-1]\n",
        "        tgt_encoded = self.vocab_tgt.encode(item['vietnamese'])\n",
        "\n",
        "        src_indices = [self.vocab_src.bos_idx] + src_encoded + [self.vocab_src.eos_idx]\n",
        "        tgt_indices = [self.vocab_tgt.bos_idx] + tgt_encoded + [self.vocab_tgt.eos_idx]\n",
        "\n",
        "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx): self.pad_idx = pad_idx\n",
        "    def __call__(self, batch):\n",
        "        src = [item[0] for item in batch]\n",
        "        trg = [item[1] for item in batch]\n",
        "        src = pad_sequence(src, batch_first=True, padding_value=self.pad_idx)\n",
        "        trg = pad_sequence(trg, batch_first=True, padding_value=self.pad_idx)\n",
        "        return src, trg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGn6My_oRlFb"
      },
      "source": [
        "Tải data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CJat4c3OUwp",
        "outputId": "d40cd56a-1c73-408d-cf8f-6f47ddbbce46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 20000 | Dev: 2000 | Test: 2000\n"
          ]
        }
      ],
      "source": [
        "train_dataset = PhoMTDataset(TRAIN_PATH, limit=20000)\n",
        "dev_dataset = PhoMTDataset(DEV_PATH, limit=2000)\n",
        "test_dataset = PhoMTDataset(TEST_PATH, limit=2000)\n",
        "\n",
        "train_dataset.build_vocabs()\n",
        "\n",
        "dev_dataset.vocab_src = train_dataset.vocab_src\n",
        "dev_dataset.vocab_tgt = train_dataset.vocab_tgt\n",
        "\n",
        "test_dataset.vocab_src = train_dataset.vocab_src\n",
        "test_dataset.vocab_tgt = train_dataset.vocab_tgt\n",
        "\n",
        "train_dataset.data.sort(key=lambda x: len(x['english'].split()))\n",
        "dev_dataset.data.sort(key=lambda x: len(x['english'].split()))\n",
        "test_dataset.data.sort(key=lambda x: len(x['english'].split()))\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Dev: {len(dev_dataset)} | Test: {len(test_dataset)}\")\n",
        "\n",
        "pad_idx = train_dataset.vocab_src.pad_idx\n",
        "collate_fn = MyCollate(pad_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK-yWZ-7Rhc-"
      },
      "source": [
        "Mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mZUx-IfOXtB"
      },
      "outputs": [],
      "source": [
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self, d_model: int, n_encoder: int, n_decoder: int, dropout: float, vocab_src, vocab_tgt):\n",
        "        super().__init__()\n",
        "        self.vocab_src = vocab_src\n",
        "        self.vocab_tgt = vocab_tgt\n",
        "\n",
        "        self.src_embedding = nn.Embedding(len(vocab_src), d_model, padding_idx=vocab_src.pad_idx)\n",
        "        self.tgt_embedding = nn.Embedding(len(vocab_tgt), d_model, padding_idx=vocab_tgt.pad_idx)\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=d_model, hidden_size=d_model, num_layers=n_encoder,\n",
        "            batch_first=True, dropout=dropout, bidirectional=False\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=d_model, hidden_size=d_model, num_layers=n_decoder,\n",
        "            batch_first=True, dropout=dropout, bidirectional=False\n",
        "        )\n",
        "\n",
        "        self.output_head = nn.Linear(d_model, len(vocab_tgt))\n",
        "        self.loss = nn.CrossEntropyLoss(ignore_index=vocab_tgt.pad_idx)\n",
        "\n",
        "    def forward_step(self, input_ids, hidden_state, cell_state):\n",
        "        embedded_input = self.tgt_embedding(input_ids)\n",
        "        output, (hidden_state, cell_state) = self.decoder(embedded_input, (hidden_state, cell_state))\n",
        "        return output, hidden_state, cell_state\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        embedded_x = self.src_embedding(x)\n",
        "        _, (hidden, cell) = self.encoder(embedded_x)\n",
        "\n",
        "        bs, tgt_len = y.shape\n",
        "        logits = []\n",
        "        curr_input = y[:, 0].unsqueeze(-1)\n",
        "\n",
        "        for ith in range(tgt_len - 1):\n",
        "            output, hidden, cell = self.forward_step(curr_input, hidden, cell)\n",
        "            logit = self.output_head(output.squeeze(1))\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "            curr_input = y[:, ith + 1].unsqueeze(-1)\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)\n",
        "        return self.loss(logits.reshape(-1, len(self.vocab_tgt)), y[:, 1:].reshape(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcUVYvO6R5e7"
      },
      "source": [
        "Tạo Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hGOD2MVOZEi"
      },
      "outputs": [],
      "source": [
        "model = Seq2seq(\n",
        "    d_model=HIDDEN_SIZE,\n",
        "    n_encoder=N_LAYERS,\n",
        "    n_decoder=N_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    vocab_src=train_dataset.vocab_src,\n",
        "    vocab_tgt=train_dataset.vocab_tgt\n",
        ").to(DEVICE)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name and param.dim() > 1:\n",
        "            nn.init.xavier_uniform_(param.data)\n",
        "        elif 'bias' in name:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erjlMzXpR8nR"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxoZHZcEOapF",
        "outputId": "a92dc493-7eef-4b29-f57e-7755eb995277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Huấn luyện 30 epochs...\n",
            "Epoch 01 | Time: 31s | Train Loss: 6.1457\n",
            "Epoch 02 | Time: 27s | Train Loss: 5.8677\n",
            "Epoch 03 | Time: 27s | Train Loss: 5.5175\n",
            "Epoch 04 | Time: 28s | Train Loss: 5.2956\n",
            "Epoch 05 | Time: 27s | Train Loss: 5.0990\n",
            "Epoch 06 | Time: 29s | Train Loss: 4.9428\n",
            "Epoch 07 | Time: 27s | Train Loss: 4.8067\n",
            "Epoch 08 | Time: 27s | Train Loss: 4.6698\n",
            "Epoch 09 | Time: 27s | Train Loss: 4.5487\n",
            "Epoch 10 | Time: 27s | Train Loss: 4.4458\n",
            "Epoch 11 | Time: 27s | Train Loss: 4.3513\n",
            "Epoch 12 | Time: 26s | Train Loss: 4.2681\n",
            "Epoch 13 | Time: 26s | Train Loss: 4.1900\n",
            "Epoch 14 | Time: 26s | Train Loss: 4.1203\n",
            "Epoch 15 | Time: 26s | Train Loss: 4.0539\n",
            "Epoch 16 | Time: 27s | Train Loss: 3.9954\n",
            "Epoch 17 | Time: 26s | Train Loss: 3.9401\n",
            "Epoch 18 | Time: 27s | Train Loss: 3.8883\n",
            "Epoch 19 | Time: 26s | Train Loss: 3.8371\n",
            "Epoch 20 | Time: 26s | Train Loss: 3.7872\n",
            "Epoch 21 | Time: 27s | Train Loss: 3.7437\n",
            "Epoch 22 | Time: 27s | Train Loss: 3.7003\n",
            "Epoch 23 | Time: 26s | Train Loss: 3.6596\n",
            "Epoch 24 | Time: 26s | Train Loss: 3.6190\n",
            "Epoch 25 | Time: 26s | Train Loss: 3.5801\n",
            "Epoch 26 | Time: 26s | Train Loss: 3.5446\n",
            "Epoch 27 | Time: 27s | Train Loss: 3.5085\n",
            "Epoch 28 | Time: 26s | Train Loss: 3.4737\n",
            "Epoch 29 | Time: 26s | Train Loss: 3.4416\n",
            "Epoch 30 | Time: 27s | Train Loss: 3.4076\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(src, tgt)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "print(f\"Huấn luyện {N_EPOCHS} epochs...\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, optimizer)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch {epoch+1:02} | Time: {end_time - start_time:.0f}s | Train Loss: {train_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euF6GSPsSAXa"
      },
      "source": [
        "Đánh giá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovTmpY7lOb5T",
        "outputId": "af37d0b9-e907-429f-892c-3b0a27685d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tính toán ROUGE-L trên tập Test\n",
            "KẾT QUẢ BÀI 1 (ROUGE-L): 0.3355\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def calculate_rouge(model, loader, dataset):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    refs = []\n",
        "\n",
        "    print(\"Tính toán ROUGE-L trên tập Test\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            src = src.to(DEVICE)\n",
        "\n",
        "            embedded_x = model.src_embedding(src)\n",
        "            _, (hidden, cell) = model.encoder(embedded_x)\n",
        "\n",
        "            bs = src.shape[0]\n",
        "            curr_input = torch.full((bs, 1), dataset.vocab_tgt.bos_idx, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "            batch_preds = []\n",
        "\n",
        "            for _ in range(50):\n",
        "                output, hidden, cell = model.forward_step(curr_input, hidden, cell)\n",
        "\n",
        "                logit = model.output_head(output.squeeze(1))\n",
        "                pred_idx = logit.argmax(dim=-1)\n",
        "\n",
        "                batch_preds.append(pred_idx.unsqueeze(1))\n",
        "                curr_input = pred_idx.unsqueeze(1)\n",
        "\n",
        "            batch_preds = torch.cat(batch_preds, dim=1)\n",
        "\n",
        "            for i in range(bs):\n",
        "                pred_tokens = []\n",
        "                for idx in batch_preds[i]:\n",
        "                    if idx == dataset.vocab_tgt.eos_idx: break\n",
        "                    pred_tokens.append(dataset.vocab_tgt.itos[idx.item()])\n",
        "\n",
        "                tgt_tokens = []\n",
        "                for idx in tgt[i]:\n",
        "                    if idx == dataset.vocab_tgt.eos_idx: break\n",
        "                    if idx not in [dataset.vocab_tgt.bos_idx, dataset.vocab_tgt.pad_idx]:\n",
        "                        tgt_tokens.append(dataset.vocab_tgt.itos[idx.item()])\n",
        "\n",
        "                preds.append(\" \".join(pred_tokens))\n",
        "                refs.append(\" \".join(tgt_tokens))\n",
        "\n",
        "    results = rouge.compute(predictions=preds, references=refs)\n",
        "    return results\n",
        "\n",
        "scores = calculate_rouge(model, test_loader, test_dataset)\n",
        "print(f\"KẾT QUẢ BÀI 1 (ROUGE-L): {scores['rougeL']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}